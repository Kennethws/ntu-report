low.scale <- lower.order %>%
mutate_all(scale)
high.scale <- higher.order %>%
mutate_all(scale)
lbp.scale <- lbp.feature %>%
as.data.frame() %>%
mutate_all(scale)
lbp.hf.scale <- lbp.hf %>%
mutate_all(scale)
lpq.uni.scale <- lpq.uni %>%
mutate_all(scale)
lpq.gaussian.scale <- lpq.gaussian %>%
mutate_all(scale)
# LPQ PCA
lpq.pca <- prcomp(lpq.gaussian.scale)
ten <- first(which(summary(lpq.pca)$importance[3,] > 0.9))
five <- first(which(summary(lpq.pca)$importance[3,] > 0.95))
twenty <- first(which(summary(lpq.pca)$importance[3,] > 0.8))
thirty <- first(which(summary(lpq.pca)$importance[3,] > 0.7))
fourty <- first(which(summary(lpq.pca)$importance[3,] > 0.6))
set.seed(924)
shuffle <- sample(nrow(hmnist64))
x <- cbind(lpq.pca$x[, 1:fourty], low.scale, high.scale, lbp.hf.scale)[shuffle,]
y <- hmnist64$label[shuffle]
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- x[test.index,]
test.y <- y[test.index] %>% factor()
train.x <- x[-test.index,]
train.y <- y[-test.index] %>% factor()
train.data <- cbind(train.x, train.y)
# logistic
logist.fit <- multinom(train.y ~ ., data = train.data,
maxit = 2000, MaxNWts = 10000000)
logist.pred <- predict(logist.fit, test.x)
mean(logist.pred == test.y)
xg.fit <- train(train.x, train.y, method = 'xgbTree',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(nrounds = 100,
max_depth = 2,
eta = .4,
gamma = 0,
colsample_bytree = .8,
min_child_weight = 1,
subsample = 1))
xg.pred <- predict(xg.fit, test.x)
mean(xg.pred == test.y)
rf.fit <- train(train.x, train.y, method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 28,
minNode = 2))
rf.pred <- predict(rf.fit, test.x)
mean(rf.pred == test.y)
# Ensemble
most <- function(v) {
tab <- table(v)
names(tab)[which.max(tab)]
}
ensemble <- cbind(logist.pred, rf.pred, xg.pred)
ensemble.pred <- apply(ensemble, 1, most) %>% factor()
mean(ensemble.pred == test.y)
#####################################################################
###------------------------------------------------------------------
### Topic : NTU Research Report
### Date  : 2020-06-17
### Author: Chao Wang
###------------------------------------------------------------------
#####################################################################
library(tidyverse)
library(broom)
library(dslabs)
library(caret)
library(nnet)
library(matrixStats)
library(klaR)
library(xtable)
library(xgboost)
library(e1071)
library(wvtool)
library(MASS)
library(mda)
library(ggrepel)
library(stargazer)
ds_theme_set()
options(digit = 3)
#--------------------------------------------------------------------
### load data
load('rdas/hmnist64.rda')
load('rdas/lower-order.rda')
load('rdas/higher-order.rda')
load('rdas/lbp-feature.rda')
load('rdas/lbp-hf.rda')
load('rdas/lpq-uniform.rda')
load('rdas/lpq-gaussian.rda')
#--------------------------------------------------------------------
## machine learning
low.scale <- lower.order %>%
mutate_all(scale)
high.scale <- higher.order %>%
mutate_all(scale)
lbp.scale <- lbp.feature %>%
as.data.frame() %>%
mutate_all(scale)
lbp.hf.scale <- lbp.hf %>%
mutate_all(scale)
lpq.uni.scale <- lpq.uni %>%
mutate_all(scale)
lpq.gaussian.scale <- lpq.gaussian %>%
mutate_all(scale)
# LPQ PCA
lpq.pca <- prcomp(lpq.gaussian.scale)
ten <- first(which(summary(lpq.pca)$importance[3,] > 0.9))
five <- first(which(summary(lpq.pca)$importance[3,] > 0.95))
twenty <- first(which(summary(lpq.pca)$importance[3,] > 0.8))
thirty <- first(which(summary(lpq.pca)$importance[3,] > 0.7))
fourty <- first(which(summary(lpq.pca)$importance[3,] > 0.6))
set.seed(924)
shuffle <- sample(nrow(hmnist64))
x <- cbind(lpq.pca$x[, 1:thirty], low.scale, high.scale, lbp.hf.scale)[shuffle,]
y <- hmnist64$label[shuffle]
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- x[test.index,]
test.y <- y[test.index] %>% factor()
train.x <- x[-test.index,]
train.y <- y[-test.index] %>% factor()
train.data <- cbind(train.x, train.y)
# logistic
logist.fit <- multinom(train.y ~ ., data = train.data,
maxit = 2000, MaxNWts = 10000000)
logist.pred <- predict(logist.fit, test.x)
mean(logist.pred == test.y)
rf.fit <- train(train.x, train.y, method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 28,
minNode = 2))
rf.pred <- predict(rf.fit, test.x)
mean(rf.pred == test.y)
xg.fit <- train(train.x, train.y, method = 'xgbTree',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(nrounds = 100,
max_depth = 2,
eta = .4,
gamma = 0,
colsample_bytree = .8,
min_child_weight = 1,
subsample = 1))
xg.pred <- predict(xg.fit, test.x)
mean(xg.pred == test.y)
rf.fit <- train(train.x, train.y, method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 86,
minNode = 2))
rf.fit <- train(train.x, train.y, method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 86,
minNode = 2))
rf.fit
rf.pred <- predict(rf.fit, test.x)
mean(rf.pred == test.y)
# Ensemble
most <- function(v) {
tab <- table(v)
names(tab)[which.max(tab)]
}
ensemble <- cbind(logist.pred, rf.pred, xg.pred)
ensemble.pred <- apply(ensemble, 1, most) %>% factor()
mean(ensemble.pred == test.y)
x <- cbind(lpq.pca$x[, 1:thirty], low.scale, high.scale)[shuffle,]
y <- hmnist64$label[shuffle]
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- x[test.index,]
test.y <- y[test.index] %>% factor()
train.x <- x[-test.index,]
train.y <- y[-test.index] %>% factor()
train.data <- cbind(train.x, train.y)
# logistic
logist.fit <- multinom(train.y ~ ., data = train.data,
maxit = 2000, MaxNWts = 10000000)
logist.pred <- predict(logist.fit, test.x)
mean(logist.pred == test.y)
rf.fit <- train(train.x, train.y, method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 28,
minNode = 2))
rf.pred <- predict(rf.fit, test.x)
mean(rf.pred == test.y)
xg.fit <- train(train.x, train.y, method = 'xgbTree',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(nrounds = 100,
max_depth = 2,
eta = .4,
gamma = 0,
colsample_bytree = .8,
min_child_weight = 1,
subsample = 1))
xg.pred <- predict(xg.fit, test.x)
mean(xg.pred == test.y)
ensemble <- cbind(logist.pred, rf.pred, xg.pred)
ensemble.pred <- apply(ensemble, 1, most) %>% factor()
mean(ensemble.pred == test.y)
confusionMatrix(ensemble.pred, test.y)
ensemble.table <- confusionMatrix(ensemble.pred, test.y)$table
xtable(ensemble.table)
ensemble.table
#####################################################################
###------------------------------------------------------------------
### Topic : NTU Research Report
### Date  : 2020-06-17
### Author: Chao Wang
###------------------------------------------------------------------
#####################################################################
library(tidyverse)
library(broom)
library(dslabs)
library(caret)
library(nnet)
library(matrixStats)
library(klaR)
library(xtable)
library(xgboost)
library(e1071)
library(wvtool)
library(MASS)
library(mda)
library(ggrepel)
library(stargazer)
ds_theme_set()
options(digit = 3)
#--------------------------------------------------------------------
### load data
load('rdas/hmnist64.rda')
load('rdas/lower-order.rda')
load('rdas/higher-order.rda')
load('rdas/lbp-feature.rda')
load('rdas/lbp-hf.rda')
load('rdas/lpq-uniform.rda')
load('rdas/lpq-gaussian.rda')
#--------------------------------------------------------------------
### analysis
## display and save sample images
si <- hmnist64 %>%
mutate(index = 1:5000) %>%
group_by(label) %>%
summarise(sample = first(index)) %>%
.$sample
tumour <- array(NA, 5000)
tumour[si] <- c('simple-stroma', 'tumour-epithelium', 'complex-stroma',
'immune-cell', 'debris', 'mucosal-gland', 'adipose-tissue',
'background')
sapply(si, function(i) {
png(paste0('figs/',tumour[i],'.png'))
image(matrix(as.matrix(hmnist64[i, -4097]), 64, 64)[,64:1],
main = tumour[i],
xaxt = 'n', yaxt = 'n')
dev.off()
})
## Preprocessing
y <- factor(hmnist64$label)
x <- hmnist64[,-4097] %>% as.matrix()
x.scale <- apply(x, 2, scale)
# column variance
sds <- colSds(x)
qplot(sds, bins = 256, fill = I('red')) +
ggtitle('Column SD of each pixel')
ggsave('figs/column-variance.png')
# all features seem to provide some info, can't remove any
nzv <- nearZeroVar(x.scale)
#--------------------------------------------------------------------
## Gray-scale machine learning
# PCA
load('rdas/pca.rda')
# memory.limit(size = 20000)
# pca <- prcomp(x.scale)
# save(pca, file = 'rdas/pca.rda')
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- pca$x[test.index,]
test.y <- y[test.index]
train.x <- pca$x[-test.index,]
train.y <- y[-test.index]
train.data <- cbind(train.x, train.y)
# plot variance
ten <- first(which(summary(pca)$importance[3,] >= 0.9))
one <- first(which(summary(pca)$importance[3,] >= 0.99))
qplot(1:ncol(train.x), summary(pca)$importance[3,],
col = I('blue'), size = I(.7)) +
xlab('number of columns') +
ylab('variance retained') +
ggtitle('variance proportion')
ggsave('figs/variance-proportion.png')
# plot PCs
qplot(PC1, PC2, data = as.data.frame(pca$x),
col = y, shape = y) +
scale_shape_manual(values = 1:nlevels(y)) +
theme(legend.title = element_blank()) +
ggtitle('PC2 vs. PC1 for eight categories')
ggsave('figs/plot-pcs.png')
# PCs boxplot
data.frame(type = y, pca$x[, 1:5]) %>%
gather(PC, value, -type) %>%
ggplot(aes(PC, value, fill = type)) +
geom_boxplot() +
theme(legend.title = element_blank()) +
ggtitle('boxplot of 5 PCs')
ggsave('figs/boxplot-5pcs.png')
# only PC1 makes a big difference, which makes sense
# then only plot PC1
data.frame(type = y, pca$x[, 1]) %>%
gather(PC, value, -type) %>%
ggplot(aes(PC, value, fill = type)) +
geom_boxplot() +
theme(legend.position = 'none') +
ggtitle('boxplot of PC1')
ggsave('figs/boxplot-1pc.png')
# looks feasible, can be separated
# extract PCs to retain 90% variance
train.data <- train.data[,c(1:ten, ncol(train.data))]
train.data <- as.data.frame(train.data)
rm(pca, x.scale)
# logistic regression
logist.fit <- multinom(train.y ~ ., data = train.data,
maxNWts = 1000000, maxit = 1500)
logist.pred <- predict(logist.fit, test.x)
confusionMatrix(logist.pred, factor(test.y))
logist.table <- confusionMatrix(logist.pred, factor(test.y))$table
logist.class <- confusionMatrix(logist.pred, factor(test.y))$byClass
stargazer(logist.class[,c(1,2,5,7)], title = 'Multinomial Logistic Regression',
align = T, type = 'text')
stargazer(logist.class[,c(1,2,5,7)], title = 'Multinomial Logistic Regression',
align = T)
# LDA
memory.limit(size = 20000)
lda.fit <- lda(train.y ~ ., data = train.data)
lda.pred <- predict(lda.fit, test.x)
confusionMatrix(lda.pred, test.y)
lda.table <- confusionMatrix(lda.pred, test.y)$table
lda.class <- confusionMatrix(lda.pred, test.y)$byClass
stargazer(lda.class[,c(1,2,5,7)], title = 'Linear Discriminant Analysis',
align = T)
stargazer(lda.class[,c(1,2,5,7)], title = 'Linear Discriminant Analysis',
align = T, type = 'text')
# QDA
qda.fit <- MASS::qda(train.y ~ ., data = train.data)
qda.pred <- predict(qda.fit, as.data.frame(test.x))
mean(qda.pred$class == test.y)
confusionMatrix(qda.pred$class, test.y)
qda.table <- confusionMatrix(qda.pred$class, test.y)$table
qda.class <- confusionMatrix(qda.pred$class, test.y)$byClass
stargazer(qda.class[,c(1,2,5,7)], title = 'Quadratic Discriminant Analysis')
stargazer(qda.class[,c(1,2,5,7)], title = 'Quadratic Discriminant Analysis',
type = 'text')
# MDA
# install.packages('mda')
library(mda)
mda.fit <- mda(train.y ~ ., data = train.data)
mda.pred <- predict(mda.fit, test.x[,1:ten])
confusionMatrix(mda.pred, test.y)
mda.table <- confusionMatrix(mda.pred, test.y)$table
mda.class <- confusionMatrix(mda.pred, test.y)$byClass
stargazer(mda.class[,c(1,2,5,7)], title = 'Mixture Discriminant Analysis')
stargazer(mda.class[,c(1,2,5,7)], title = 'Mixture Discriminant Analysis',
type = 'text')
# FDA
library(mda)
fda.fit <- fda(train.y ~ ., data = train.data)
fda.pred <- predict(fda.fit, test.x[,1:ten])
confusionMatrix(fda.pred, test.y)
fda.class <- confusionMatrix(fda.pred, test.y)$byClass
stargazer(fda.class[,c(1,2,5,7)], title = 'Flexible Discriminant Analysis')
stargazer(fda.class[,c(1,2,5,7)], title = 'Flexible Discriminant Analysis',
type = 'text')
# RDA
# install.packages('klaR')
library(klaR)
rda.fit <- rda(train.y ~ ., data = train.data)
#####################################################################
###------------------------------------------------------------------
### Topic : NTU Research Report
### Date  : 2020-06-17
### Author: Chao Wang
###------------------------------------------------------------------
#####################################################################
library(tidyverse)
library(broom)
library(dslabs)
library(caret)
library(nnet)
library(matrixStats)
library(klaR)
library(xtable)
library(xgboost)
library(e1071)
library(wvtool)
library(MASS)
library(mda)
library(ggrepel)
library(stargazer)
ds_theme_set()
options(digit = 3)
#--------------------------------------------------------------------
### load data
load('rdas/hmnist64.rda')
load('rdas/lower-order.rda')
load('rdas/higher-order.rda')
load('rdas/lbp-feature.rda')
load('rdas/lbp-hf.rda')
load('rdas/lpq-uniform.rda')
load('rdas/lpq-gaussian.rda')
#--------------------------------------------------------------------
## machine learning
low.scale <- lower.order %>%
mutate_all(scale)
high.scale <- higher.order %>%
mutate_all(scale)
lbp.scale <- lbp.feature %>%
as.data.frame() %>%
mutate_all(scale)
lbp.hf.scale <- lbp.hf %>%
mutate_all(scale)
lpq.uni.scale <- lpq.uni %>%
mutate_all(scale)
lpq.gaussian.scale <- lpq.gaussian %>%
mutate_all(scale)
# LPQ PCA
lpq.pca <- prcomp(lpq.gaussian.scale)
ten <- first(which(summary(lpq.pca)$importance[3,] > 0.9))
five <- first(which(summary(lpq.pca)$importance[3,] > 0.95))
twenty <- first(which(summary(lpq.pca)$importance[3,] > 0.8))
thirty <- first(which(summary(lpq.pca)$importance[3,] > 0.7))
fourty <- first(which(summary(lpq.pca)$importance[3,] > 0.6))
set.seed(924)
shuffle <- sample(nrow(hmnist64))
x <- cbind(lpq.pca$x[, 1:thirty], low.scale)[shuffle,]
y <- hmnist64$label[shuffle]
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- x[test.index,]
test.y <- y[test.index] %>% factor()
train.x <- x[-test.index,]
train.y <- y[-test.index] %>% factor()
train.data <- cbind(train.x, train.y)
# learning curve
svm.linear.data <- learning_curve_dat(train.data, outcome = 'train.y',
method = 'svmLinear',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(C = 1))
p1 <- svm.linear.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, color = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of Linear SVM')
p1
x <- cbind(lpq.pca$x[, 1:thirty], low.scale, high.scale, lbp.hf.scale)[shuffle,]
y <- hmnist64$label[shuffle]
set.seed(924)
test.index <- createDataPartition(y, times = 1, p = .1, list = F)
test.x <- x[test.index,]
test.y <- y[test.index] %>% factor()
train.x <- x[-test.index,]
train.y <- y[-test.index] %>% factor()
train.data <- cbind(train.x, train.y)
# learning curve
svm.linear.data <- learning_curve_dat(train.data, outcome = 'train.y',
method = 'svmLinear',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(C = 1))
p1 <- svm.linear.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, color = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of Linear SVM')
p1
# learning curve
svm.rb.data <- learning_curve_dat(train.data,
outcome = 'train.y',
trControl = trainControl('cv', 10),
method = 'svmRadial',
tuneGrid = expand.grid(sigma = 0.004502533,
C = 2))
svm.rb.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, col = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of rb SVM')
p2 <- svm.rb.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, col = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of rb SVM')
# learning curve
rf.data <- learning_curve_dat(train.data, outcome = 'train.y',
method = 'Rborist',
trControl = trainControl('cv', 10),
tuneGrid = expand.grid(predFixed = 28,
minNode = 2))
rf.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, color = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of Random Forest')
p3 <- rf.data %>%
ggplot(aes(Training_Size, 1 - Accuracy, color = Data)) +
geom_smooth(method = loess, span = .8) +
ylab('Error') +
ggtitle('Learning Curve of Random Forest')
library(ggpubr)
# aligned plots
ggarrange(p1, p2, p3,
labels = c('A', 'B', 'C'),
common.legend = T, legend = 'bottom')
